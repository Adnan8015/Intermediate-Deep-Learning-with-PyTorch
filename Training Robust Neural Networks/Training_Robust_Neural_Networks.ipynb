{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch and object-oriented programming"
      ],
      "metadata": {
        "id": "MD1vFPNrqvgU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PyTorch Dataset**"
      ],
      "metadata": {
        "id": "065vkdm6qsyp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sb61_vTioS22"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "\n",
        "To train a model, we need to build a PyTorch Dataset, set up a DataLoader, and define the model.\n",
        "\n",
        "We start with the init method, which reads a CSV file into a DataFrame and stores it in the data attribute as a NumPy array.\n",
        "The super-init command ensures our WaterDataset class behaves like its parent class, torch Dataset.\n",
        "\n",
        "Next, PyTorch requires us to implement the len method that returns the total size of the dataset which we access as the 0th element of DataFrame's shape.\n",
        "\n",
        "Finally, we add the getitem method, which takes one argument called idx, the index of a sample, and returns the features (all columns but the last one)\n",
        "and the label (the final column) for that sample.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class WaterDataset(Dataset):\n",
        "    def __init__(self, csv_path):\n",
        "        super().__init__()\n",
        "        df = pd.read_csv(csv_path)\n",
        "        self.data = df.to_numpy()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        features = self.data[idx, :-1]\n",
        "        label = self.data[idx, -1]\n",
        "        return features, label"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pytorch Dataloader**"
      ],
      "metadata": {
        "id": "qJZOPjwbrdA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "With the WaterDataset class defined, we create an instance of the Dataset, passing it the training data file path.\n",
        "\n",
        "Then, we pass the Dataset to the PyTorch DataLoader, setting the batch size to two and shuffling the training samples randomly.\n",
        "\n",
        "\n",
        "We use the next-iter-combination to get one batch from the DataLoader. With a batch size of two, we get two samples,\n",
        "each consisting of nine features and a target label.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "dataset_train = WaterDataset(\n",
        "      \"water_train.csv\"\n",
        "    )\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataloader_train = DataLoader(\n",
        "      dataset_train,\n",
        "      batch_size=2,\n",
        "      shuffle=True,\n",
        "  )\n",
        "\n",
        "features, labels = next(iter(dataloader_train))\n",
        "print(f\"Features: {features},\\nLabels: {labels}\")"
      ],
      "metadata": {
        "id": "IcADG6RIraQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PyTorch Model**"
      ],
      "metadata": {
        "id": "GlkV_H2fsP52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "PyTorch models are also best defined as classes. We may have seen sequential models defined like this before. That's fine for small models,\n",
        "but using classes gives us more flexibility to customize as complexity grows.\n",
        "\n",
        "We can rewrite this model using OOP. The Net class is based on the nn.Module, PyTorch's base class for neural networks.\n",
        "We define the model layers we want to use in the init method.\n",
        "\n",
        "The forward method describes what happens to the input when passed to the model.\n",
        "Here, we pass it through subsequent layers that we defined in the init method and wrap each layer's output in the activation function\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(9, 16)\n",
        "        self.fc2 = nn.Linear(16, 8)\n",
        "        self.fc3 = nn.Linear(8, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = nn.functional.sigmoid(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "net = Net()"
      ],
      "metadata": {
        "id": "gjY4YKGpsCsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizers, training, and evaluation"
      ],
      "metadata": {
        "id": "tUDFnoSas7Zb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training loop**"
      ],
      "metadata": {
        "id": "kERVt8znvv9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(1000):\n",
        "  for features, labels in dataloader_train:\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(features)\n",
        "    loss = criterion(\n",
        "    outputs, labels.view(-1, 1) ### reshape the labels with the view method to match the shape of the outputs.\n",
        "    )\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "KzjVrkrzs-vf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stochastic Gradient Descent (SGD)**"
      ],
      "metadata": {
        "id": "5heFjDX5w0My"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "In Stochastic Gradient Descent, or SGD, the size of the parameter update depends only on the learning rate, a predefined hyperparameter.\n",
        "SGD is computationally efficient, but because of its simplicity, it's rarely used in practice.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "j3LjiPRgw2Wo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adaptive Gradient (Adagrad)**"
      ],
      "metadata": {
        "id": "AmuyAp1RxCAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Using the same learning rate for each parameter cannot be optimal. Adaptive Gradient, or Adagrad, improves on it\n",
        "by decreasing the learning rate during training for parameters that are infrequently updated.\n",
        "\n",
        "This makes it well-suited for sparse data, that is, data in which some features are not often observed.\n",
        "However, Adagrad tends to decrease the learning rate too fast.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "optimizer = optim.Adagrad(net.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "4sjE8yRNxET_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Root Mean Square Propagation (RMSprop)**"
      ],
      "metadata": {
        "id": "YAZLqJ8HxWRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Root Mean Square Propagation, or RMSprop, addresses Adagrad's aggressive learning rate decay by adapting the learning rate for each parameter\n",
        "based on the size of its previous gradients.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "optimizer = optim.RMSprop(net.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "Y5rrG8cFxaVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adaptive Moment Estimation (Adam)**"
      ],
      "metadata": {
        "id": "bWr_bsc4xmwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Finally, Adaptive Moment Estimation or Adam is arguably the most versatile and widely used optimizer.\n",
        "It combines RMSprop with the concept of momentum: the average of past gradients where the most recent gradients have more weight.\n",
        "\n",
        "Basing the update on both gradient size and momentum helps accelerate training. Adam is often the default go-to optimizer.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "MTQMMy70xoaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model evaluation**"
      ],
      "metadata": {
        "id": "4rKf7lQGyF4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Once the model is trained, we can evaluate its performance on test data\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from torchmetrics import Accuracy\n",
        "acc = Accuracy(task=\"binary\")\n",
        "net.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for features, labels in dataloader_test:\n",
        "      outputs = net(features)\n",
        "      preds = (outputs >= 0.5).float()\n",
        "      acc(preds, labels.view(-1, 1))\n",
        "\n",
        "accuracy = acc.compute()\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "Z7gQH5I5yHK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vanishing and exploding gradients"
      ],
      "metadata": {
        "id": "VU7JYRS6y7DD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vanishing Gradients**"
      ],
      "metadata": {
        "id": "sHYwe23Y0e3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Neural networks often suffer from gradient instability during training. Sometimes, the gradients get smaller during the backward pass.\n",
        "This is known as vanishing gradients. As a result, earlier layers receive hardly any parameter updates and the model doesn't learn.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "VCHgxTPEy9kC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exploding Gradients**"
      ],
      "metadata": {
        "id": "msOSNTvf0pVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "In other cases, the gradients get increasingly large, leading to huge parameter updates and divergent training. This is known as exploding gradients.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "fgP1PZuX0tUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Solution to unstable gradients**"
      ],
      "metadata": {
        "id": "aHUSmBQA3H3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "To address these problems, we need a three-step solution consisting of proper weights initialization, good activations, and batch normalization\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "WbLPaSGD3L30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Weights Initialization**"
      ],
      "metadata": {
        "id": "lLiKXFyR3RSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Good initialization ensures:\n",
        "    Variance of layer inputs = variance of layer outputs\n",
        "    Variance of gradients the same before and after a layer\n",
        "\n",
        "The way to achieve this is different for each activation function. For ReLU, or Rectified Linear Unit, and similar activations, we can use He initialization,\n",
        "also known as Kaiming initialization\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "layer = nn.Linear(8, 1)\n",
        "print(layer.weight)\n",
        "\n",
        "\n",
        "\n",
        "import torch.nn.init as init\n",
        "init.kaiming_uniform_(layer.weight)\n",
        "print(layer.weight)"
      ],
      "metadata": {
        "id": "ThdHp3Wj3Trd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**He / Kaiming initialization**"
      ],
      "metadata": {
        "id": "bUDYD0Sh5up0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "      self.fc1 = nn.Linear(9, 16)\n",
        "      self.fc2 = nn.Linear(16, 8)\n",
        "      self.fc3 = nn.Linear(8, 1)\n",
        "\n",
        "      init.kaiming_uniform_(self.fc1.weight)\n",
        "      init.kaiming_uniform_(self.fc2.weight)\n",
        "      init.kaiming_uniform_(\n",
        "          self.fc3.weight,\n",
        "          nonlinearity=\"sigmoid\",\n",
        "      )"
      ],
      "metadata": {
        "id": "lMri1W6z5vOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Activation functions**"
      ],
      "metadata": {
        "id": "GrwAbvaj6TpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "The ReLU, or Rectified Linear Unit, is arguably the most commonly used activation. It's available as nn.functional.relu.\n",
        "It has several advantages, but also an important drawback. It suffers from the dying neuron problem: during training, some neurons only output a zero.\n",
        "This is caused by the fact that ReLU is zero for any negative value. If inputs to a neuron become negative, it effectively dies.\n",
        "\n",
        "\n",
        "The ELU or Exponential Linear Unit is one activation designed to improve upon ReLU. It's available as nn.functional.elu.\n",
        "Thanks to non-zero gradients for negative values, it doesn't suffer from the dying neurons problem. Additionally, its average output is near zero,\n",
        "so it's less prone to vanishing gradients.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "wjKiMfU26U8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Batch Normalization**"
      ],
      "metadata": {
        "id": "5uEaIAF28LK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        " After a layer:\n",
        "    1. Normalize the layer's outputs by:\n",
        "             Subtracting the mean\n",
        "             Dividing by the standard deviation\n",
        "    2. Scale and shift normalized outputs using learned parameters\n",
        "\n",
        "\"\"\"\n",
        "\n",
        " class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(9, 16)\n",
        "        self.bn1 = nn.BatchNorm1d(16)\n",
        "        ...\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = nn.functional.elu(x)"
      ],
      "metadata": {
        "id": "7VHaH9ev8OBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Call the He (Kaiming) initializer on the weight attribute of the second layer, fc2, similarly to how it's done for fc1.\n",
        "Call the He (Kaiming) initializer on the weight attribute of the third layer, fc3, accounting for the different activation function used in the final layer.\n",
        "Update the activation functions in the forward() method from relu to elu.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(9, 16)\n",
        "        self.fc2 = nn.Linear(16, 8)\n",
        "        self.fc3 = nn.Linear(8, 1)\n",
        "\n",
        "        # Apply He initialization\n",
        "        init.kaiming_uniform_(self.fc1.weight)\n",
        "        init.kaiming_uniform_(self.fc2.weight)\n",
        "        init.kaiming_uniform_(\n",
        "            self.fc3.weight,\n",
        "            nonlinearity = \"sigmoid\",\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Update ReLU activation to ELU\n",
        "        x = nn.functional.elu(self.fc1(x))\n",
        "        x = nn.functional.elu(self.fc2(x))\n",
        "        x = nn.functional.sigmoid(self.fc3(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "a0tHcj1Q8duz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Add two BatchNorm1d layers assigning them to self.bn1 and self.bn2\n",
        "\n",
        "In the forward() method, pass x through the second set of layers: the linear layer, the batch norm layer, and the activations, similarly to how it's done for the first set of layers\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(9, 16)\n",
        "        # Add two batch normalization layers\n",
        "        self.bn1 = nn.BatchNorm1d(16)\n",
        "        self.fc2 = nn.Linear(16, 8)\n",
        "        self.bn2 = nn.BatchNorm1d(8)\n",
        "        self.fc3 = nn.Linear(8, 1)\n",
        "\n",
        "        init.kaiming_uniform_(self.fc1.weight)\n",
        "        init.kaiming_uniform_(self.fc2.weight)\n",
        "        init.kaiming_uniform_(self.fc3.weight, nonlinearity=\"sigmoid\")"
      ],
      "metadata": {
        "id": "Ermt_5nT8m3o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}