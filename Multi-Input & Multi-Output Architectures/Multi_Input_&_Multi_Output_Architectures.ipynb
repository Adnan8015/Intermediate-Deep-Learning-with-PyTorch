{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Input Models"
      ],
      "metadata": {
        "id": "bVCj8MEsnAQ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why multi-input?**"
      ],
      "metadata": {
        "id": "Vi8e8XhEnEYX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mv3eCEndmjVw"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "\n",
        "Multi-input models, or models that accept more than one source of data, have many applications.\n",
        "    First, we might want the model to use multiple information sources, such as two images of the same car to predict its model.\n",
        "\n",
        "    Second, multi-modal models can work on different input types such as image and text to answer a question about the image.\n",
        "\n",
        "    Next, in metric learning, the model learns whether two inputs represent the same object.\n",
        "Think about an automated passport control where the system compares our passport photo with a picture it takes of us.\n",
        "\n",
        "    Finally, in self-supervised learning, the model learns data representation by learning that two augmented versions of the same input represent the same object.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Omniglot Dataset**"
      ],
      "metadata": {
        "id": "iNXr7u0en97u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Omniglot dataset, a collection of images of 964 different handwritten characters from 30 different alphabets.\n",
        "\n",
        "The first input will be the image of the character, such as this Latin letter \"k\".\n",
        "The second input will the the alphabet that it comes from expressed as a one-hot vector.\n",
        "\n",
        "Both inputs will be processed separately, then we concatenate their representations.Finally a classification layer predicts one of the 964 classes.\n",
        "We need two elements to build such a model: a custom Dataset and an appropriate model architecture\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "oMs7Gxj5oAF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Two-Input Dataset**"
      ],
      "metadata": {
        "id": "QT7pzQtgrBoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "class OmniglotDataset(Dataset):\n",
        "    def __init__(self, transform, samples):\n",
        "        self.transform = transform\n",
        "        self.samples = samples ### Samples are tuples of three: image file path, alphabet as a one-hot vector, and target label as the character class index\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, alphabet, label = self.samples[idx]\n",
        "        img = Image.open(img_path).convert('L')  ### The convert method with the argument \"L\" makes sure that the image is read as grayscale\n",
        "        img = self.transform(img)\n",
        "        return img, alphabet, label"
      ],
      "metadata": {
        "id": "Imzi7mhzrGFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Two-Input architecture**"
      ],
      "metadata": {
        "id": "uTIOz55Xr3x4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.image_layer = nn.Sequential(\n",
        "        nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
        "        nn.MaxPool2d(kernel_size=2),\n",
        "        nn.ELU(),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(16*32*32, 128)\n",
        "        )\n",
        "\n",
        "        self.alphabet_layer = nn.Sequential(\n",
        "        nn.Linear(30, 8), ### Its input size is 30, the number of alphabets, and we map it to an arbitrarily chosen output size of 8\n",
        "        nn.ELU(),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "        nn.Linear(128 + 8, 964),\n",
        "        )\n",
        "\n",
        "     def forward(self, x_image, x_alphabet):\n",
        "        x_image = self.image_layer(x_image)\n",
        "        x_alphabet = self.alphabet_layer(x_alphabet)\n",
        "        x = torch.cat((x_image, x_alphabet), dim=1)\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "In the forward method, we pass each input through its corresponding layer. Then, we concatenate the outputs with torch.cat.\n",
        "Finally, we pass the result through the classifier layer and return.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ZM4OfcF8rhzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Loop**"
      ],
      "metadata": {
        "id": "rDl-CyBvs8-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(10):\n",
        "    for img, alpha, labels in dataloader_train:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(img, alpha)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "BeYCnsp7s_uC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Building a multi-input model starts with crafting a custom dataset that can supply all the inputs to the model.\n",
        "In this exercise, you will build the Omniglot dataset that serves triplets consisting of:\n",
        "\n",
        "The image of a character to be classified,\n",
        "The one-hot encoded alphabet vector of length 30, with zeros everywhere but for a single one denoting the ID of the alphabet the character comes from,\n",
        "The target label, an integer between 0 and 963.\n",
        "You are provided with samples, a list of 3-tuples comprising an image's file path, its alphabet vector, and the target label.\n",
        "Also, the following imports have already been done for you.\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Assign transform and samples to class attributes with the same names\n",
        "\n",
        "Implement the .__len()__ method such that it returns the number of samples stored in the class' samples attribute\n",
        "\n",
        "Unpack the sample at index idx assigning its contents to img_path, alphabet, and label.\n",
        "Transform the loaded image with self.transform() and assign it to img_transformed.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class OmniglotDataset(Dataset):\n",
        "    def __init__(self, transform, samples):\n",
        "        # Assign transform and samples to class attributes\n",
        "        self.transform = transforms\n",
        "        self.samples = samples\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return number of samples\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      \t# Unpack the sample at index idx\n",
        "        img_path, alphabet, label = self.samples[idx]\n",
        "        img = Image.open(img_path).convert('L')\n",
        "        # Transform the image\n",
        "        img_transformed = self.transform(img)\n",
        "        return img_transformed, alphabet, label\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### With implementation of OmniglotDataset ready, you can actually create the dataset and DataLoader, just like you did it before\n",
        "# dataset_train = OmniglotDataset(\n",
        "#     transform=transforms.Compose([\n",
        "#         transforms.ToTensor(),\n",
        "#         transforms.Resize((64, 64)),\n",
        "#     ]),\n",
        "#     samples=samples,\n",
        "# )\n",
        "\n",
        "# dataloader_train = DataLoader(\n",
        "#     dataset_train, shuffle=True, batch_size=3,\n",
        "# )"
      ],
      "metadata": {
        "id": "X-kSXXqxtdp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Define image, alphabet and classifier sub-networks as sequential models, assigning them to self.image_layer, self.alphabet_layer and self.classifier, respectively.\n",
        "\n",
        "Pass the image and alphabet through the appropriate model layers.\n",
        "\n",
        "Concatenate the outputs from image and alphabet layers and assign the result to x.\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Define sub-networks as sequential models\n",
        "        self.image_layer = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.ELU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(16*32*32, 128)\n",
        "        )\n",
        "        self.alphabet_layer = nn.Sequential(\n",
        "            nn.Linear(30, 8),\n",
        "            nn.ELU(),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(128 + 8, 964),\n",
        "        )\n",
        "\n",
        "    def forward(self, x_image, x_alphabet):\n",
        "        # Pass the x_image and x_alphabet through appropriate layers\n",
        "        x_image = self.image_layer(x_image)\n",
        "        x_alphabet = self.alphabet_layer(x_alphabet)\n",
        "        # Concatenate x_image and x_alphabet\n",
        "        x = torch.cat((x_image, x_alphabet), dim=1)\n",
        "        return self.classifier(x)"
      ],
      "metadata": {
        "id": "NZXkbduivSYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-output Models"
      ],
      "metadata": {
        "id": "I0n7L66SrCge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why multi-output?**"
      ],
      "metadata": {
        "id": "2yozE0_vrVA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Just like multi-input models, multi-output architectures are everywhere. Their simplest use-case is for multi-task learning,\n",
        "where we want to predict two things from the same input, such as a car's make and model from its picture.\n",
        "In multi-label classification problem, the input can belong to multiple classes simultaneously. For instance, an image can depict both a beach and people.\n",
        "For each of these labels, a separate output from the model is needed.\n",
        "\n",
        "Finally, in very deep models built of blocks of layers, it is a common practice to add extra outputs predicting the same targets after each block.\n",
        "These additional outputs ensure that the early parts of the model are learning features useful for the task\n",
        "at hand while also serving as a form of regularization to boost the robustness of the network.\n",
        "\n",
        "\n",
        "\"\""
      ],
      "metadata": {
        "id": "RWmGBGDkrWoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "use the Omniglot dataset again to build a model to predict both the character(A, B, ... ..., ---> Total 964) and the alphabet(English, Bangla, Latin,..., ---> Total 30) it comes from based on the image.\n",
        "First, we will pass the image through some layers to obtain its embedding.\n",
        "\n",
        "Then we add two independent classifiers on top, one for each output.\n",
        "\n",
        "\n",
        "Changing the Dataset Format\n",
        "-----------------------------------\n",
        "Before, when we used the alphabet as an input, we represented it using a one-hot vector (a list with mostly zeros and a single 1).\n",
        "Now, since the alphabet is an output, we just use an integer label (e.g., 0 for first alphabet, 1 for second, and so on up to 29).\n",
        "\n",
        "Before: Alphabet 'Greek' → [0, 1, 0, ..., 0] (one-hot vector).\n",
        "Now: Alphabet 'Greek' → 1 (just a single number).\n",
        "\n",
        "\n",
        "\n",
        "Forward Pass (Prediction Process)\n",
        "------------------------------------\n",
        "\n",
        "When we pass an image into the model:\n",
        "\n",
        "The image processing layers extract features.\n",
        "These features go into two separate classifier layers.\n",
        "Each classifier makes a prediction (one for character, one for alphabet).\n",
        "Finally, the model outputs both predictions.\n",
        "\n",
        "\n",
        "Example: Suppose we give the model an image of a handwritten character.\n",
        "\n",
        "It processes the image and extracts features.\n",
        "The character classifier predicts: \"This is character 340.\"\n",
        "The alphabet classifier predicts: \"This belongs to alphabet 5.\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class OmniglotDataset(Dataset):\n",
        "    def __init__(self, transform, samples):\n",
        "        self.transform = transform\n",
        "        self.samples = samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, alphabet, label = \\\n",
        "        self.samples[idx]\n",
        "        img = Image.open(img_path).convert('L')\n",
        "        img = self.transform(img)\n",
        "        return img, alphabet, label\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, num_alpha, num_char):\n",
        "        super().__init__()\n",
        "        self.image_layer = nn.Sequential(\n",
        "          nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
        "          nn.MaxPool2d(kernel_size=2),\n",
        "          nn.ELU(),\n",
        "          nn.Flatten(),\n",
        "          nn.Linear(16*32*32, 128)\n",
        "        )\n",
        "\n",
        "        self.classifier_alpha = nn.Linear(128, 30)\n",
        "        self.classifier_char = nn.Linear(128, 964)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_image = self.image_layer(x)\n",
        "        output_alpha = self.classifier_alpha(x_image)\n",
        "        output_char = self.classifier_char(x_image)\n",
        "        return output_alpha, output_char"
      ],
      "metadata": {
        "id": "m689Yh1krFYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Loop**"
      ],
      "metadata": {
        "id": "MBtCs0oSQrgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10):\n",
        "    for images, labels_alpha, labels_char in dataloader_train:\n",
        "        optimizer.zero_grad()\n",
        "        outputs_alpha, outputs_char = net(images)\n",
        "\n",
        "        loss_alpha = criterion(\n",
        "        outputs_alpha, labels_alpha\n",
        "        )\n",
        "\n",
        "        loss_char = criterion(\n",
        "        outputs_char, labels_char\n",
        "        )\n",
        "\n",
        "        loss = loss_alpha + loss_char\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "MMrRP-JxQuXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Use your OmniglotDataset to create dataset_train, passing the two image transforms you have used before: parse the image to a tensor and resize it to size (64, 64).\n",
        "\n",
        "Create dataloader_train from dataset_train; shuffle the training images and set batch size to 32.\n",
        "\"\"\"\n",
        "\n",
        "# Print the sample at index 100\n",
        "print(samples[100])\n",
        "\n",
        "# Create dataset_train\n",
        "dataset_train = OmniglotDataset(\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "      \ttransforms.Resize((64, 64)),\n",
        "    ]),\n",
        "    samples=samples,\n",
        ")\n",
        "\n",
        "# Create dataloader_train\n",
        "dataloader_train = DataLoader(\n",
        "    dataset_train, shuffle=True, batch_size= 32,\n",
        ")"
      ],
      "metadata": {
        "id": "ncAbBtICi8QY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Define self.classifier_alpha and self.classifier_char as linear layers with input shapes matching the output of image_layer,\n",
        "and output shapes corresponding to the number of alphabets (30) and the number of characters (964), respectively.\n",
        "\n",
        "\n",
        "Pass the image embedding x_image separately through each of the classifiers, assigning the results to output_alpha and output_char, respectively,\n",
        "and return them in this order\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.image_layer = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.ELU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(16*32*32, 128)\n",
        "        )\n",
        "\n",
        "        # Define the two classifier layers\n",
        "        self.classifier_alpha = nn.Linear(128, 30)\n",
        "        self.classifier_char = nn.Linear(128, 964)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_image = self.image_layer(x)\n",
        "        # Pass x_image through the classifiers and return both results\n",
        "        output_alpha = self.classifier_alpha(x_image)\n",
        "        output_char = self.classifier_char(x_image)\n",
        "        return output_alpha , output_char\n"
      ],
      "metadata": {
        "id": "RO44qPRdjybt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Calculate the alphabet classification loss and assign it to loss_alpha.\n",
        "Calculate the character classification loss and assign it to loss_char.\n",
        "Compute the total loss as the sum of the two partial losses and assign it to loss.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "net = Net()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.05)\n",
        "\n",
        "for epoch in range(1):\n",
        "    for images, labels_alpha, labels_char in dataloader_train:\n",
        "        optimizer.zero_grad()\n",
        "        outputs_alpha, outputs_char = net(images)\n",
        "        # Compute alphabet classification loss\n",
        "        loss_alpha = criterion(\n",
        "            outputs_alpha, labels_alpha\n",
        "        )\n",
        "        # Compute character classification loss\n",
        "        loss_char = criterion(\n",
        "             outputs_char, labels_char\n",
        "        )\n",
        "        # Compute total loss\n",
        "        loss = loss_alpha + loss_char\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "ydB077w5kjAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation of multi output models and loss weighting"
      ],
      "metadata": {
        "id": "FPkXQ1JkmBvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "We chose to define the final loss as the sum of the two partial losses. By doing so, we are telling the model that\n",
        "recognizing characters and recognizing alphabets are equally important to us. If that is not the case, we can combine the two losses differently.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "WynaZEXlmF9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Warning: losses on different scales**"
      ],
      "metadata": {
        "id": "OtLsHSnSunbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "There is just one caveat: when assigning loss weights, we must be aware of the magnitudes of the loss values.\n",
        "If the losses are not on the same scale, one loss could dominate the other, causing the model to effectively ignore the smaller loss.\n",
        "Consider a scenario where we're building a model to predict house prices, and use MSE loss.\n",
        "\n",
        "If we also want to use the same model to provide a quality assessment of the house, categorized as \"Low\", \"Medium\", or \"High\", we would use cross-entropy loss.\n",
        "\n",
        "Cross-entropy is typically in the single-digit range, while MSE can reach tens of thousands.\n",
        "Combining these two would result in the model ignoring the quality assessment task completely.\n",
        "A solution is to scale each loss by dividing it by the maximum value in the batch.\n",
        "This brings them to the same range, allowing us to weight them if desired and add together.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "K6aWROWHupJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Define acc_alpha and acc_char as multi-class Accuracy() metrics for the two outputs, alphabets and characters, with the appropriate number of classes each\n",
        "(there are 30 alphabets and 964 characters in the dataset).\n",
        "\n",
        "\n",
        "Define the evaluation loop by iterating over test images, labels_alpha, and labels_char.\n",
        "Inside the for-loop, obtain model results for the test data batch and assign them to outputs_alpha, outputs_char.\n",
        "\n",
        "\n",
        "Update the two accuracy metrics with the current batch's data.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def evaluate_model(model):\n",
        "    # Define accuracy metrics\n",
        "    acc_alpha = Accuracy(task=\"multiclass\", num_classes=30)\n",
        "    acc_char = Accuracy(task=\"multiclass\", num_classes=964)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for images, labels_alpha, labels_char in dataloader_test:\n",
        "            # Obtain model outputs\n",
        "            outputs_alpha, outputs_char = model(images)\n",
        "            _, pred_alpha = torch.max(outputs_alpha, 1)\n",
        "            _, pred_char = torch.max(outputs_char, 1)\n",
        "            acc_alpha(pred_alpha, labels_alpha)\n",
        "            acc_char(pred_char, labels_char)\n",
        "\n",
        "    print(f\"Alphabet: {acc_alpha.compute()}\")\n",
        "    print(f\"Character: {acc_char.compute()}\")"
      ],
      "metadata": {
        "id": "S6dkMnpEuZbU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}