{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Train-test split in sequential data**"
      ],
      "metadata": {
        "id": "-YWw9vlHgpYg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYKQSw62f6NE"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "\n",
        "In many machine learning applications, one randomly splits the data into training and testing sets. However, with sequential data, there are better approaches.\n",
        "If we split the data randomly, we risk creating a look-ahead bias, where the model has information about the future when making forecasts.\n",
        "In practice, we won't have information about the future when making predictions, so our test set should reflect this reality.\n",
        "To avoid the look-ahead bias, we should split the data by time. We will train on the first three years of data, and test on the fourth year\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating Sequences**"
      ],
      "metadata": {
        "id": "RutNlAqzh0aR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "To feed the training data to the model, we need to chunk it first to create sequences that the model can use as training examples.\n",
        "First, we need to select the sequence length, which is the number of data points in one training example. Let's make each forecast based on the previous 24 hours.\n",
        "Because data is at 15 minute intervals, we need to use 24 times 4 which is 96 data points.\n",
        "In each example, the data point right after the input sequence will be the target to predict\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "R7gxHYzNh2sV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generating Sequences**"
      ],
      "metadata": {
        "id": "ZrWm88acierB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Iterate over the range of the number of data points minus the length of an input sequence.\n",
        "\n",
        "Define the inputs x as the slice of df from the ith row to the i + seq_lengthth row and the column at index 1.\n",
        "\n",
        "Define the target y as the slice of df at row index i + seq_length and the column at index 1.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def create_sequences(df, seq_length):   ### The loop goes from the first row (i = 0) to the last row where a full sequence can be made\n",
        "    xs, ys = [], []\n",
        "    # Iterate over data indices\n",
        "    for i in range(len(df) - seq_length):\n",
        "      \t# Define inputs\n",
        "        x = df.iloc[i : (i + seq_length), 1]\n",
        "        # Define target\n",
        "        y = df.iloc[(i + seq_length), 1]\n",
        "        xs.append(x)\n",
        "        ys.append(y)\n",
        "    return np.array(xs), np.array(ys)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Index\t      Timestamp\t            Consumption\n",
        "0\t      2011-01-01 00:15:00\t      -0.70\n",
        "1\t      2011-01-01 00:30:00\t      -0.70\n",
        "2\t      2011-01-01 00:45:00\t      -0.68\n",
        "3\t      2011-01-01 01:00:00\t      -0.66\n",
        "4\t      2011-01-01 01:15:00     \t-0.65\n",
        "5\t      2011-01-01 01:30:00     \t-0.63\n",
        "\n",
        "Sequence Length (seq_length): 3\n",
        "\n",
        "\n",
        "Iteration 1 (i = 0):\n",
        "---------------------\n",
        "Sequence (x):\n",
        "------------\n",
        "df.iloc[0:3, 1]  # Rows 0, 1, 2 → [-0.70, -0.70, -0.68]\n",
        "Sequence: [-0.70, -0.70, -0.68]\n",
        "\n",
        "\n",
        "Target (y):\n",
        "-----------\n",
        "df.iloc[3, 1]  # Row 3 → -0.66\n",
        "Target: -0.66\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "osXiS5n2ibfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sequential Dataset**"
      ],
      "metadata": {
        "id": "K5hTneviiv6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Just like tabular and image data, sequential data is easiest passed to a model through a torch Dataset and DataLoader.\n",
        "To build a sequential Dataset, you will call create_sequences() to get the NumPy arrays with inputs and targets, and inspect their shape.\n",
        "Next, you will pass them to a TensorDataset to create a proper torch Dataset, and inspect its length.\n",
        "\n",
        "Your implementation of create_sequences() and a DataFrame with the training data called train_data are available\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Call create_sequences(), passing it the training DataFrame and a sequence length of 24*4, assigning the result to X_train, y_train.\n",
        "\n",
        "Define \"dataset_train\" by calling TensorDataset and passing it two arguments, the inputs and the targets created by create_sequences(), both converted from NumPy arrays to tensors of floats.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "# Use create_sequences to create inputs and targets\n",
        "X_train, y_train = create_sequences(train_data , seq_length = 96)\n",
        "print(X_train.shape, y_train.shape)\n",
        "\n",
        "# Create TensorDataset\n",
        "dataset_train = TensorDataset(\n",
        "    torch.from_numpy(X_train).float(),\n",
        "    torch.from_numpy(y_train).float(),\n",
        ")\n",
        "print(len(dataset_train))\n",
        "\n",
        "\n",
        "\n",
        "##### The TensorDataset we have just built behaves the same way as other Torch Datasets we have used before, such us our custom WaterDataset or the ImageFolder dataset; we can pass it to a DataLoader in the same way."
      ],
      "metadata": {
        "id": "sW_eFns1iynu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Recurrent Neural Networks**"
      ],
      "metadata": {
        "id": "587yU3quz2oj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Define the RNN layer passing it the correct values for input_size, hidden_size, num_layers, and batch_first, and assign it to self.rnn\n",
        "\n",
        "Initialize the first hidden state h0 as a tensor of zeros of the appropriate shape.\n",
        "\n",
        "Pass the input x and the first hidden state h0 through recurrent layer.\n",
        "\n",
        "Pass recurrent layer's last output through the linear layer\n",
        "\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Define RNN layer\n",
        "        self.rnn = nn.RNN(\n",
        "            input_size = 1,\n",
        "            hidden_size = 32,\n",
        "            num_layers = 2,\n",
        "            batch_first = True,\n",
        "        )\n",
        "        self.fc = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize first hidden state with zeros\n",
        "        h0 = torch.zeros(2, x.size(0), 32)\n",
        "        # Pass x and h0 through recurrent layer\n",
        "        out, _ = self.rnn(x , h0)\n",
        "        # Pass recurrent layer's last output through linear layer\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ],
      "metadata": {
        "id": "PHNNE8uYx8K9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "In our dataset model learns to predict the sum of the last two numbers in a sequence.\n",
        "\n",
        "For example:\n",
        "\n",
        "Input: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "Output: 19 (because 9 + 10 = 19)\n",
        "\n",
        "\n",
        "## (1). Generating the Data ; We create sequences of length 10, where the model should predict the sum of the last two values.\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Toy dataset: Sequence where the target is the sum of last two numbers\n",
        "def generate_sequences(num_samples=4, seq_length=10):\n",
        "    X = []  # Input sequences\n",
        "    Y = []  # Target values\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        seq = torch.randint(1, 20, (seq_length,)).float()  # Random sequence of numbers\n",
        "        X.append(seq.unsqueeze(1))  # Make it shape (seq_length, 1)\n",
        "        Y.append(seq[-2:].sum().unsqueeze(0))  # Target = Sum of last two numbers\n",
        "\n",
        "    return torch.stack(X), torch.stack(Y)\n",
        "\n",
        "# Generate 4 sequences\n",
        "X_train, Y_train = generate_sequences(num_samples=4, seq_length=10)\n",
        "\n",
        "print(\"Input X (Sequences):\")\n",
        "print(X_train)  # Shape (4, 10, 1) ; (batch_size, sequence_length, input_size(Each timestep has only one feature) )\n",
        "print(\"Target Y (Sum of last two numbers):\")\n",
        "print(Y_train)  # Shape (4, 1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## (2). Running the Model with This Data\n",
        "\n",
        "\n",
        "# Define the RNN Model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.RNN(input_size=1, hidden_size=32, num_layers=2, batch_first=True)\n",
        "        self.fc = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(2, x.size(0), 32)  # (num_layers=2, batch_size, hidden_size=32)\n",
        "        out, _ = self.rnn(x, h0)  # Forward pass through RNN\n",
        "        out = self.fc(out[:, -1, :])  # Take last timestep output\n",
        "        return out\n",
        "\n",
        "# Create model\n",
        "model = Net()\n",
        "\n",
        "# Forward pass with real data\n",
        "output = model(X_train)\n",
        "\n",
        "print(\"\\nModel Predictions:\")\n",
        "print(output)\n",
        "\n",
        "\n",
        "\n",
        "## (3). Example Output\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Input X (Sequences):\n",
        "tensor([[[ 8.], [10.], [19.], [10.], [ 3.], [ 1.], [12.], [ 4.], [ 3.], [ 5.]],\n",
        "        [[18.], [ 2.], [ 9.], [ 5.], [ 4.], [ 7.], [14.], [19.], [14.], [ 5.]],\n",
        "        [[10.], [ 6.], [15.], [17.], [14.], [ 3.], [18.], [19.], [19.], [ 2.]],\n",
        "        [[ 2.], [12.], [10.], [ 3.], [10.], [19.], [ 3.], [ 2.], [11.], [ 4.]]])\n",
        "\n",
        "Target Y (Sum of last two numbers):\n",
        "tensor([[ 8.],  # 3+5\n",
        "        [19.],  # 14+5\n",
        "        [21.],  # 19+2\n",
        "        [15.]]) # 11+4\n",
        "\n",
        "Model Predictions:\n",
        "tensor([[10.5732],\n",
        "        [15.0432],\n",
        "        [17.2898],\n",
        "        [12.3023]])\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "1qdW877jx8Z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM and GRU Cell"
      ],
      "metadata": {
        "id": "HbyMt9uSaoIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "In the .__init__() method, define an LSTM layer and assign it to self.lstm.\n",
        "In the forward() method, initialize the first long-term memory hidden state c0 with zeros.\n",
        "In the forward() method, pass all three inputs to the LSTM layer: the current time step's inputs, and a tuple containing the two hidden states\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super().__init__()\n",
        "        # Define lstm layer\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=1,\n",
        "            hidden_size=32,\n",
        "            num_layers=2,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.fc = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(2, x.size(0), 32)\n",
        "        # Initialize long-term memory\n",
        "        c0 = torch.zeros(2, x.size(0), 32)\n",
        "        # Pass all inputs to lstm layer\n",
        "        out, _ = self.lstm(x , (h0 , c0))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ],
      "metadata": {
        "id": "knm5zmNzauCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Update the RNN model definition in order to obtain a GRU network; assign the GRU layer to self.gru.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Define RNN layer\n",
        "        self.rnn = nn.GRU(\n",
        "            input_size=1,\n",
        "            hidden_size=32,\n",
        "            num_layers=2,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.fc = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(2, x.size(0), 32)\n",
        "        out, _ = self.gru(x, h0)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ],
      "metadata": {
        "id": "AJzRQhTWa7DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Evaluating RNNs"
      ],
      "metadata": {
        "id": "ULL8A3MHbFLt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expanding tensors**"
      ],
      "metadata": {
        "id": "g3MgfcLwbxfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "All recurrent layers, RNNs, LSTMs, and GRUs, expect input in the shape: batch size, sequence length, number of features.\n",
        "\n",
        "But as we loop over the DataLoader, we can see that we got the shape batch size of 32 by the sequence length of 96.\n",
        "Since we are dealing with only one feature, the electricity consumption, the last dimension is dropped.\n",
        "We can add it, or expand the tensor, by calling view on the sequence and passing the desired shape.\n",
        "\n",
        "\n",
        "\n",
        "for seqs, labels in dataloader_train:\n",
        "      print(seqs.shape) ### Output : torch.Size([32, 96])\n",
        "\n",
        "seqs = seqs.view(32, 96, 1)\n",
        "print(seqs.shape)   ### Output : torch.Size([32, 96, 1])\n",
        "\"\"\"\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "snOqtWqbbIEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Squeezing Tensors**"
      ],
      "metadata": {
        "id": "sg2kUZyIcmfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Conversely, as we evaluate the model, we will need to revert the expansion we have applied to the model inputs which can be achieved through squeezing.\n",
        "As we iterate through test data batches, we get labels in shape batch size. Model outputs, however, are of shape batch size by 1, our number of features.\n",
        "\n",
        "We will be passing the labels and the model outputs to the loss function, and each PyTorch loss requires its inputs to be of the same shape.\n",
        "To achieve that, we can apply the squeeze method to the model outputs. This will reshape them to match the labels' shape\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "fzfl5i9YcqKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Loop**"
      ],
      "metadata": {
        "id": "p4hr7d-8h-iv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(\n",
        "    net.parameters(), lr=0.001\n",
        ")\n",
        "for epoch in range(num_epochs):\n",
        "    for seqs, labels in dataloader_train:\n",
        "        seqs = seqs.view(32, 96, 1)\n",
        "        outputs = net(seqs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "nTDBFOX3iBMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation Loop**"
      ],
      "metadata": {
        "id": "q8e_9XC3iSdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mse = torchmetrics.MeanSquaredError()\n",
        "\n",
        "net.eval()\n",
        "with torch.no_grad():\n",
        "    for seqs, labels in test_loader:\n",
        "        seqs = seqs.view(32, 96, 1)\n",
        "        outputs = net(seqs).squeeze()\n",
        "        mse(outputs, labels)\n",
        "\n",
        "print(f\"Test MSE: {mse.compute()}\")"
      ],
      "metadata": {
        "id": "6-ExL-OoiVIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Set up the Mean Squared Error loss and assign it to criterion.\n",
        "Reshape seqs to (batch size, sequence length, num features), which in our case is (32, 96, 1), and re-assign the result to seqs.\n",
        "Pass seqs to the model to get its outputs\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "net = Net()\n",
        "# Set up MSE loss\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(\n",
        "  net.parameters(), lr=0.0001\n",
        ")\n",
        "\n",
        "for epoch in range(3):\n",
        "    for seqs, labels in dataloader_train:\n",
        "        # Reshape model inputs\n",
        "        seqs = seqs.view(32, 96, 1)\n",
        "        # Get model outputs\n",
        "        outputs = net(seqs)\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "id": "M7y8TMOykG7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Define the Mean Squared Error metrics and assign it to mse.\n",
        "Pass the input sequence to net, and squeeze the result before you assign it to outputs.\n",
        "Compute the final value of the test metric assigning it to test_mse.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Define MSE metric\n",
        "mse = torchmetrics.MeanSquaredError()\n",
        "\n",
        "net.eval()\n",
        "with torch.no_grad():\n",
        "    for seqs, labels in dataloader_test:\n",
        "        seqs = seqs.view(32, 96, 1)\n",
        "        # Pass seqs to net and squeeze the result\n",
        "        outputs = net(seqs).squeeze()\n",
        "        mse(outputs, labels)\n",
        "\n",
        "# Compute final metric value\n",
        "test_mse = mse.compute()\n",
        "print(f\"Test MSE: {test_mse}\")\n"
      ],
      "metadata": {
        "id": "sd-A_XzlkQPe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}